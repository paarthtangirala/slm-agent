version: '3.8'

services:
  # SLM Personal Agent Application
  slm-agent:
    build: .
    container_name: slm-personal-agent
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=phi3:mini
      - SERPAPI_KEY=${SERPAPI_KEY}
      - BING_SEARCH_KEY=${BING_SEARCH_KEY}
      - SEARCH_PROVIDER=serpapi
    volumes:
      - ./uploads:/app/uploads
      - ./chromadb:/app/chromadb
      - ./conversations.db:/app/conversations.db
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - slm-network

  # Ollama LLM Server
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - slm-network
    # Uncomment if you have GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Optional: Redis for caching (future enhancement)
  # redis:
  #   image: redis:7-alpine
  #   container_name: slm-redis
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis-data:/data
  #   restart: unless-stopped
  #   networks:
  #     - slm-network

  # Optional: Nginx reverse proxy for production
  nginx:
    image: nginx:alpine
    container_name: slm-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - slm-agent
    restart: unless-stopped
    networks:
      - slm-network

volumes:
  ollama-data:
  # redis-data:

networks:
  slm-network:
    driver: bridge